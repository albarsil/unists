{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python\\python36\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph\n",
    "from rdflib import URIRef\n",
    "from rdflib.namespace import RDF\n",
    "from rdflib import Namespace\n",
    "import nltk\n",
    "import codecs\n",
    "import csv\n",
    "from nltk.corpus import wordnet as wn\n",
    "from num2words import num2words\n",
    "from nltk.stem.snowball import PortugueseStemmer\n",
    "from xml.dom import minidom\n",
    "import string\n",
    "import pickle\n",
    "import numpy\n",
    "import nltk\n",
    "import re\n",
    "import json\n",
    "import codecs\n",
    "from nltk.stem.snowball import PortugueseStemmer\n",
    "from xml.dom import minidom\n",
    "import string\n",
    "import pickle\n",
    "import numpy\n",
    "import nltk\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from num2words import num2words\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PortugueseStemmer()\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "word_tokenize = nltk.word_tokenize\n",
    "punct = string.punctuation\n",
    "\n",
    "def expand_synonyms(tokens, it=1):\n",
    "    \"\"\"Docstring.\"\"\"\n",
    "    def recursion(tokens):\n",
    "        new = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sin = tep[delaf[token]]\n",
    "                if len(sin) < 2:\n",
    "                    new += sin\n",
    "            except:\n",
    "                pass\n",
    "        return list(set(tokens + new))\n",
    "    for i in range(it):\n",
    "        tokens = recursion(tokens)\n",
    "    return tokens\n",
    "\n",
    "def word2embeddings(tokens):\n",
    "    \"\"\"Docstring.\"\"\"\n",
    "    return [embeddings[emb_vocab.index(t)]\n",
    "            if t in emb_vocab else embeddings[0] for t in tokens]\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"Docstring.\"\"\"\n",
    "    return [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "def filter_stopwords(tokens):\n",
    "    \"\"\"Docstring.\"\"\"\n",
    "    return [i.lower() for i in tokens if\n",
    "            i.lower() not in stopwords and i not in punct]\n",
    "\n",
    "def get_palavras_words(tree):\n",
    "    \"\"\"Docstring.\"\"\"\n",
    "    tree = tree.replace('\\\\n', '\\n')\n",
    "    return re.findall('\\[(.*)\\]', tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_synonyms(word, language, deep):\n",
    "    synonyms = _find_synonyms(word, language, deep, 0)\n",
    "    synonyms = list(set(synonyms))\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_synonyms(word, language, deep, actual_deep):   \n",
    "    synonyms = []\n",
    "    if(deep == actual_deep):\n",
    "        return [word]\n",
    "    else:\n",
    "        for ss in wn.synsets(word, lang=language):\n",
    "            for lemma in ss.lemma_names(lang=language):\n",
    "                synonyms.extend(_find_synonyms(lemma, language, deep, actual_deep + 1))\n",
    "        return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hypernyms(word, language, deep):\n",
    "    hypernyms = _find_hypernyms(word, language, deep, 0)\n",
    "    hypernyms = list(set(hypernyms))\n",
    "    return hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_hypernyms(word, language, deep, actual_deep):   \n",
    "    hypernyms = []\n",
    "    if(deep == actual_deep):\n",
    "        return [word]\n",
    "    else:\n",
    "        for ss in wn.synsets(word, lang=language):\n",
    "            for h in ss.hypernyms():\n",
    "                for lemma in h.lemma_names(lang=language):\n",
    "                    hypernyms.extend(_find_hypernyms(lemma, language, deep, actual_deep + 1))\n",
    "        return hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hyponyms(word, language, deep):\n",
    "    hyponyms = _find_hyponyms(word, language, deep, 0)\n",
    "    hyponyms = list(set(hyponyms))\n",
    "    return hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_hyponyms(word, language, deep, actual_deep):   \n",
    "    hyponyms = []\n",
    "    if(deep == actual_deep):\n",
    "        return [word]\n",
    "    else:\n",
    "        for ss in wn.synsets(word, lang=language):\n",
    "            for h in ss.hyponyms():\n",
    "                for lemma in h.lemma_names(lang=language):\n",
    "                    hyponyms.extend(_find_hyponyms(lemma, language, deep, actual_deep + 1))\n",
    "        return hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_synonyms('cachorro', 'por', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_hypernyms('cachorro', 'por', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_hyponyms('cachorro', 'por', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_word_synonyms(sentence1, sentence2, language, synonym_deep = 2):\n",
    "    sentence1 = sentence1.split()\n",
    "    sentence2 = sentence2.split()\n",
    "    \n",
    "    sentence2_grams = ['_'.join(x) for x in nltk.ngrams(sentence2, 3)]\n",
    "    \n",
    "    lt = []\n",
    "    \n",
    "    for t in sentence1:\n",
    "        if t in sentence2:\n",
    "            lt.append(t)\n",
    "        else:\n",
    "            ths = find_synonyms(t, language, synonym_deep)\n",
    "            ts = t\n",
    "            for th in ths:\n",
    "                if th in sentence2 or th in sentence2_grams:\n",
    "                    ts = th\n",
    "                    break\n",
    "            lt.append(ts)\n",
    "                    \n",
    "    return ' '.join(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_word_hyponym(sentence1, sentence2, language, hyponym_deep = 2):\n",
    "    sentence1 = sentence1.split()\n",
    "    sentence2 = sentence2.split()\n",
    "    \n",
    "    sentence2_grams = ['_'.join(x) for x in nltk.ngrams(sentence2, 3)]\n",
    "    \n",
    "    lt = []\n",
    "    \n",
    "    for t in sentence1:\n",
    "        if t in sentence2:\n",
    "            lt.append(t)\n",
    "        else:\n",
    "            ths = find_hyponyms(t, language, hyponym_deep)\n",
    "            ts = t\n",
    "            for th in ths:\n",
    "                if th in sentence2 or th in sentence2_grams:\n",
    "                    ts = th\n",
    "                    break\n",
    "            lt.append(ts)\n",
    "                    \n",
    "    return ' '.join(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_word_hypernyms(sentence1, sentence2, language, hyperonym_deep = 2):\n",
    "    sentence1 = sentence1.split()\n",
    "    sentence2 = sentence2.split()\n",
    "    \n",
    "    sentence2_grams = ['_'.join(x) for x in nltk.ngrams(sentence2, 3)]\n",
    "    \n",
    "    lt = []\n",
    "    \n",
    "    for t in sentence1:\n",
    "        if t in sentence2:\n",
    "            lt.append(t)\n",
    "        else:\n",
    "            ths = find_hypernyms(t, language, hyperonym_deep)\n",
    "            ts = t\n",
    "            for th in ths:\n",
    "                if th in sentence2 or th in sentence2_grams:\n",
    "                    ts = th\n",
    "                    break\n",
    "            lt.append(ts)\n",
    "                    \n",
    "    return ' '.join(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_word_hypernyms_of_synonyms(sentence1, sentence2, language, synonym_deep = 2, hyperonym_deep = 2):   \n",
    "    sentence1 = sentence1.split()\n",
    "    sentence2 = sentence2.split()\n",
    "    \n",
    "    sentence2_grams = ['_'.join(x) for x in nltk.ngrams(sentence2, 3)]\n",
    "    \n",
    "    lt = []\n",
    "    \n",
    "    for t in sentence1:\n",
    "        if t in sentence2:\n",
    "            lt.append(t)\n",
    "        else:\n",
    "            reserved_words = find_synonyms(t, language, synonym_deep)\n",
    "            reserved_words.append([find_hypernyms(x, language, hyperonym_deep) for x in reserved_words])\n",
    "            \n",
    "            changed_word = t\n",
    "            for x in reserved_words:\n",
    "                if x in sentence2 or x in sentence2_grams:\n",
    "                    changed_word = x.replace('_', ' ')\n",
    "                    \n",
    "            lt.append(changed_word)\n",
    "                    \n",
    "    return ' '.join(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_word_synonyms('o jogo de futebol foi confirmar excelente hoje e um cachorro invadiu o campo', 'o inter afirmar jogou bem no campeonato, mas houve um cão invadiu o campo', 'por')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_word_hypernyms_of_synonyms('o jogo de futebol foi confirmar excelente hoje e um cachorro invadiu o campo', 'o inter afirmar jogou bem no campeonato, mas houve um cão invadiu o campo', 'por')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number_tryexcept(s):\n",
    "    \"\"\" Returns True is string is a number. \"\"\"\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_to_word(word, language):\n",
    "\n",
    "    if language == 'por':\n",
    "        language = 'pt_BR'\n",
    "    elif language == 'eng':\n",
    "        language = 'en'\n",
    "\n",
    "    try:\n",
    "        return num2words(float(word), to = 'cardinal', lang = language)\n",
    "    except NotImplementedError:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_to_word('2013', 'por')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseSentenceFile_synonyms(df, output, lang = 'por', to_json = False):\n",
    "    data = []\n",
    "    for index, row in df.iterrows():\n",
    "        new = {}\n",
    "        t1 = row['t']\n",
    "        t2 = row['h']\n",
    "\n",
    "        # Remove stopwords and tokenize\n",
    "        t1 = filter_stopwords(nltk.word_tokenize(t1))\n",
    "        t2 = filter_stopwords(nltk.word_tokenize(t2))\n",
    "        \n",
    "        # Remove spaces and to lower\n",
    "        t1 = [t.lower().strip() for t in t1]\n",
    "        t2 = [t.lower().strip() for t in t2]\n",
    "\n",
    "        # Convert numbers to words\n",
    "        t1 = [number_to_word(s.strip(), lang).replace('vírgula zero', '') if is_number_tryexcept(s.strip()) else s for s in t1]\n",
    "        t2 = [number_to_word(s.strip(), lang).replace('vírgula zero', '') if is_number_tryexcept(s.strip()) else s for s in t2]\n",
    "\n",
    "        t1 = ' '.join(t1)\n",
    "        t2 = ' '.join(t2)\n",
    "               \n",
    "        # Replace hyperonyms\n",
    "        # pth1 = [k if k in nltk.word_tokenize(t2) else t for k in find_hypernyms(t, lang) if t in nltk.word_tokenize(t2) else t for t in nltk.word_tokenize(t1)]\n",
    "        # pth2 = [k if k in nltk.word_tokenize(t1) else t for k in find_hypernyms(t, lang) if t in nltk.word_tokenize(t1) else t for t in nltk.word_tokenize(t2)]\n",
    "        pth1 = replace_word_synonyms(t1, t2, lang, 3)\n",
    "        pth2 = replace_word_synonyms(t2, t1, lang, 3)\n",
    "\n",
    "        new['t'] = pth1\n",
    "        new['h'] = pth2\n",
    "        \n",
    "        # Similaridade apenas para conjunto de treinamento\n",
    "        try:\n",
    "            new['entailment'] = row['entailment']\n",
    "            new['similarity'] = row['similarity']\n",
    "        except:\n",
    "            pass\n",
    "        data.append(new)\n",
    "\n",
    "    with codecs.open(output, \"wb\", encoding='utf8', errors='ignore') as fp:\n",
    "        if to_json == True:\n",
    "            json.dump(data, fp, ensure_ascii=False)\n",
    "        else:\n",
    "            pickle.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseSentenceFile_hyperonyms(df, output, lang = 'por', to_json = False):\n",
    "    data = []\n",
    "    for index, row in df.iterrows():\n",
    "        new = {}\n",
    "        t1 = row['t']\n",
    "        t2 = row['h']\n",
    "\n",
    "        # Remove stopwords and tokenize\n",
    "        t1 = filter_stopwords(nltk.word_tokenize(t1))\n",
    "        t2 = filter_stopwords(nltk.word_tokenize(t2))\n",
    "        \n",
    "        # Remove spaces and to lower\n",
    "        t1 = [t.lower().strip() for t in t1]\n",
    "        t2 = [t.lower().strip() for t in t2]\n",
    "\n",
    "        # Convert numbers to words\n",
    "        t1 = [number_to_word(s.strip(), lang).replace('vírgula zero', '') if is_number_tryexcept(s.strip()) else s for s in t1]\n",
    "        t2 = [number_to_word(s.strip(), lang).replace('vírgula zero', '') if is_number_tryexcept(s.strip()) else s for s in t2]\n",
    "               \n",
    "        # Replace hyperonyms\n",
    "        # pth1 = [k if k in nltk.word_tokenize(t2) else t for k in find_hypernyms(t, lang) if t in nltk.word_tokenize(t2) else t for t in nltk.word_tokenize(t1)]\n",
    "        # pth2 = [k if k in nltk.word_tokenize(t1) else t for k in find_hypernyms(t, lang) if t in nltk.word_tokenize(t1) else t for t in nltk.word_tokenize(t2)]\n",
    "        pth1 = replace_word_hypernyms(t1, t2, lang, 3)\n",
    "        pth2 = replace_word_hypernyms(t2, t1, lang, 3)\n",
    "\n",
    "        new['t'] = pth1\n",
    "        new['h'] = pth2\n",
    "\n",
    "        # Similaridade apenas para conjunto de treinamento\n",
    "        try:\n",
    "            new['entailment'] = row['entailment']\n",
    "            new['similarity'] = row['similarity']\n",
    "        except:\n",
    "            pass\n",
    "        data.append(new)\n",
    "\n",
    "    with codecs.open(output, \"wb\", encoding='utf8', errors='ignore') as fp:\n",
    "        if to_json == True:\n",
    "            json.dump(data, fp, ensure_ascii=False)\n",
    "        else:\n",
    "            pickle.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseSentenceFile_synonyms_hyperonyms(df, output, lang = 'por', to_json = False):\n",
    "    data = []\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        new = {}\n",
    "        t1 = row['t']\n",
    "        t2 = row['h']\n",
    "\n",
    "        \n",
    "        # Remove stopwords and tokenize\n",
    "        t1 = filter_stopwords(nltk.word_tokenize(t1))\n",
    "        t2 = filter_stopwords(nltk.word_tokenize(t2))\n",
    "        \n",
    "        # Remove spaces and to lower\n",
    "        t1 = [t.lower().strip() for t in t1]\n",
    "        t2 = [t.lower().strip() for t in t2]\n",
    "\n",
    "        # Convert numbers to words\n",
    "        t1 = [number_to_word(s.strip(), lang).replace('vírgula zero', '') if is_number_tryexcept(s.strip()) else s for s in t1]\n",
    "        t2 = [number_to_word(s.strip(), lang).replace('vírgula zero', '') if is_number_tryexcept(s.strip()) else s for s in t2]\n",
    "\n",
    "        t1 = ' '.join(t1)\n",
    "        t2 = ' '.join(t2)\n",
    "        \n",
    "        \n",
    "        pts1 = replace_word_hypernyms_of_synonyms(t1, t2, lang, 3)\n",
    "        pts2 = replace_word_hypernyms_of_synonyms(t2, t1, lang, 3)\n",
    "              \n",
    "        new['t'] = pts1\n",
    "        new['h'] = pts2\n",
    "\n",
    "        # Similaridade apenas para conjunto de treinamento\n",
    "        try:\n",
    "            new['entailment'] = row['entailment']\n",
    "            new['similarity'] = row['similarity']\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        data.append(new)\n",
    "\n",
    "    with codecs.open(output, \"wb\", encoding='utf8', errors='ignore') as fp:\n",
    "        if to_json == True:\n",
    "            json.dump(data, fp, ensure_ascii=False)\n",
    "        else:\n",
    "            pickle.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xml_propor_dataset(input):\n",
    "    df = pd.DataFrame(columns=['t', 'h', 'entailment', 'similarity'])\n",
    "    index = 0\n",
    "    xml = minidom.parse(input)\n",
    "    pairs = xml.getElementsByTagName('pair')\n",
    "    data = []\n",
    "    for i in pairs:\n",
    "        t = i.getElementsByTagName('t')[0].childNodes[0].data.strip()\n",
    "        h = i.getElementsByTagName('h')[0].childNodes[0].data.strip()\n",
    "        ent = i.attributes['entailment'].value\n",
    "        sim = i.attributes['similarity'].value\n",
    "        \n",
    "        index = index + 1\n",
    "        df.loc[index] = [t, h, ent, sim]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_propor_dataset(input):\n",
    "    with codecs.open(input, 'rb', encoding='utf8') as fr:\n",
    "        df_json = json.load(fr)\n",
    "    \n",
    "    index = 0\n",
    "    df = pd.DataFrame(columns=['t', 'h', 'entailment', 'similarity'])\n",
    "    for x in df_json:\n",
    "        if isinstance(x, dict):\n",
    "            x = list(x.values())\n",
    "            \n",
    "        t = x[0]\n",
    "        h = x[1]\n",
    "        ent = x[2]\n",
    "        sim = x[3]\n",
    "        \n",
    "        index = index + 1\n",
    "        df.loc[index] = [t, h, ent, sim]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Portuguese sentences parsing **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseSentenceFile_synonyms(\n",
    "        read_json_propor_dataset('C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/setP1.json'),\n",
    "         'C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/linguistic/setP1-synonym.json',\n",
    "         'por',\n",
    "         True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseSentenceFile_synonyms(\n",
    "        read_json_propor_dataset('C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/setP2.json'),\n",
    "         'C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/linguistic/setP2-synonym.json',\n",
    "         'por',\n",
    "         True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseSentenceFile_synonyms(\n",
    "        read_json_propor_dataset('C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/setP3.json'),\n",
    "         'C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/linguistic/setP3-synonym.json',\n",
    "         'por',\n",
    "         True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseSentenceFile_synonyms_hyperonyms(\n",
    "        read_json_propor_dataset('C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/setP1.json'),\n",
    "         'C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/linguistic/setP1-synonym-hyperonym.json',\n",
    "         'por',\n",
    "         True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseSentenceFile_synonyms_hyperonyms(\n",
    "        read_json_propor_dataset('C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/setP2.json'),\n",
    "         'C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/linguistic/setP2-synonym-hyperonym.json',\n",
    "         'por',\n",
    "         True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseSentenceFile_synonyms_hyperonyms(\n",
    "        read_json_propor_dataset('C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/setP3.json'),\n",
    "         'C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/linguistic/setP3-synonym-hyperonym.json',\n",
    "         'por',\n",
    "         True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** English sentences parsing **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseSentenceFile_synonyms(\n",
    "        read_json_propor_dataset('C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/setE1.json'),\n",
    "         'C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/linguistic/setE1-synonym.json',\n",
    "         'eng',\n",
    "         True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseSentenceFile_synonyms(\n",
    "        read_json_propor_dataset('C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/setE2.json'),\n",
    "         'C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/linguistic/setE2-synonym.json',\n",
    "         'eng',\n",
    "         True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseSentenceFile_synonyms(\n",
    "        read_json_propor_dataset('C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/setE3.json'),\n",
    "         'C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/linguistic/setE3-synonym.json',\n",
    "         'eng',\n",
    "         True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseSentenceFile_synonyms_hyperonyms(\n",
    "        read_json_propor_dataset('C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/setE1.json'),\n",
    "         'C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/linguistic/setE1-synonym-hyperonym.json',\n",
    "         'eng',\n",
    "         True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseSentenceFile_synonyms_hyperonyms(\n",
    "        read_json_propor_dataset('C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/setE2.json'),\n",
    "         'C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/linguistic/setE2-synonym-hyperonym.json',\n",
    "         'eng',\n",
    "         True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseSentenceFile_synonyms_hyperonyms(\n",
    "        read_json_propor_dataset('C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/setE3.json'),\n",
    "         'C:/Users/Asilva/Documents/Git/unists-python/Data/splitted/linguistic/setE3-synonym-hyperonym.json',\n",
    "         'eng',\n",
    "         True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = KeyedVectors.load_word2vec_format('C:/Users/Asilva/Documents/Git/unists-python/Data/glove_s1000/glove_s1000.txt', unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RSLPStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer.lemmatize('fiz', pos='v', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/Asilva/Desktop/dados-cursos.csv', sep=\";\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('C:/Users/Asilva/Desktop/dados-cursos.json', 'w', encoding='utf8') as filew:\n",
    "    filew.write(df.to_json(orient='records', lines=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('C:/Users/Asilva/Desktop/dados-cursos-readable.csv', sep=\";\",quotechar='\"',index=False,\n",
    "            quoting=csv.QUOTE_ALL, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
